# 第七章：自然语言理解 - 练习题

## 一、选择题（每题2分，共20分）

1. 自然语言理解的基本流程是：
   A. 语法分析→词法分析→语义分析→语用分析
   B. 词法分析→语法分析→语义分析→语用分析
   C. 语义分析→语法分析→词法分析→语用分析
   D. 语用分析→语义分析→语法分析→词法分析

2. 下列哪项不属于词法分析的内容？
   A. 词性标注
   B. 分词
   C. 词形还原
   D. 句法分析

3. TF-IDF中的IDF表示：
   A. 词频
   B. 文档频率
   C. 逆文档频率
   D. 文档长度

4. 在自然语言处理中，词嵌入（Word Embedding）的主要目的是：
   A. 压缩文本
   B. 将词语转换为向量表示
   C. 提高存储效率
   D. 简化分词过程

5. 下列哪项是语义分析的任务？
   A. 句法结构分析
   B. 词性标注
   C. 词义消歧
   D. 分词

6. 命名实体识别（NER）主要识别的内容不包括：
   A. 人名
   B. 地名
   C. 组织机构名
   D. 动词

7. 语言模型的目的是：
   A. 翻译语言
   B. 计算句子概率
   C. 生成语法树
   D. 分析词性

8. 在神经网络语言模型中，Softmax函数的作用是：
   A. 增加非线性
   B. 对输出进行归一化
   C. 提高准确性
   D. 加快训练速度

9. 下列哪种模型最适合处理序列到序列的任务？
   A. CNN
   B. RNN
   C. Seq2Seq
   D. SVM

10. 注意力机制（Attention）主要解决的问题是：
    A. 计算速度慢
    B. 长序列信息丢失
    C. 内存占用大
    D. 模型复杂度高

## 二、填空题（每空2分，共20分）

1. 自然语言处理的四个主要分析层次是________、________、________和________。

2. 中文分词的主要方法有________、________和________。

3. 常见的词嵌入方法包括________、________和________。

4. 语义分析的主要任务有________、________、________等。

5. 机器翻译的基本方法包括________、________和________。

6. 自然语言生成的几个阶段包括________、________、________。

## 三、计算题（每题10分，共30分）

1. 计算文档中词"学习"的TF-IDF值。文档包含10个词，"学习"出现3次。在语料库的1000个文档中，包含"学习"的文档有100个。

2. 对于句子"小明喜欢学习人工智能"，请进行分词和词性标注（假设词典为：小明-nr，喜欢-v，学习-vn，人工智能-n）。

3. 已知一个小型语料库包含3个句子：[我爱自然语言处理]，[自然语言处理很有趣]，[深度学习是自然语言的重要方法]。使用2-gram模型计算句子"自然语言处理很有趣"的概率。

## 四、简答题（每题10分，共20分）

1. 解释自然语言处理中的词向量表示方法，并说明其相对于传统方法的优势。

2. 比较规则方法和统计方法在自然语言处理中的优缺点。

## 五、应用题（每题10分，共10分）

1. 设计一个中文文本分类系统，要求包括：预处理流程、特征提取方法、模型选择、评估指标和优化策略。

---
# 第七章：自然语言理解 - 详细答案

## 一、选择题答案

1. **答案：B**
   详解：NLP处理的层次结构是词法→语法→语义→语用。

2. **答案：D**
   详解：句法分析属于语法分析范畴，不属于词法分析。

3. **答案：C**
   详解：IDF是Inverse Document Frequency，逆文档频率。

4. **答案：B**
   详解：词嵌入将离散的词语转换为连续的向量表示。

5. **答案：C**
   详解：词义消歧是语义分析的任务。

6. **答案：D**
   详解：NER主要识别名词性实体，不包括动词。

7. **答案：B**
   详解：语言模型用于计算句子或词序列的概率。

8. **答案：B**
   详解：Softmax将输出转化为概率分布，进行归一化。

9. **答案：C**
   详解：Seq2Seq模型专门处理序列到序列任务。

10. **答案：B**
    详解：注意力机制解决RNN处理长序列时信息丢失问题。

## 二、填空题答案

1. **答案：** 词法分析、语法分析、语义分析、语用分析
    详解：NLP分析的四个层次。

2. **答案：** 基于词典的方法、基于统计的方法、基于深度学习的方法
    详解：中文分词的主要方法。

3. **答案：** Word2Vec、GloVe、FastText
    详解：常见的词嵌入方法。

4. **答案：** 词义消歧、语义角色标注、句法语义分析
    详解：语义分析的任务。

5. **答案：** 基于规则的翻译、基于统计的翻译、基于神经网络的翻译
    详解：机器翻译方法。

6. **答案：** 内容确定、句子规划、文本实现
    详解：自然语言生成阶段。

## 三、计算题答案

### 1. TF-IDF计算

**答案：**
- 词频 TF = 词在文档中出现次数 / 文档总词数 = 3/10 = 0.3
- 逆文档频率 IDF = log(总文档数 / 包含该词的文档数) = log(1000/100) = log(10) = 1 (以自然对数为底约为2.3，以10为底为1)
  
如果使用自然对数：IDF = ln(10) ≈ 2.30

TF-IDF = TF × IDF = 0.3 × 2.30 ≈ 0.69

**答案：** TF-IDF ≈ 0.69

**详解：** TF-IDF通过词频和逆文档频率的乘积来衡量词的重要性。

### 2. 分词和词性标注

**答案：**
分词结果：小明 / 喜欢 / 学习 / 人工智能

词性标注结果：
- 小明 - nr (人名)
- 喜欢 - v (动词) 
- 学习 - vn (动名词)
- 人工智能 - n (名词)

**详解：** 分词是将连续文本切分为词汇单元，词性标注是标注每个词的语法类别。

### 3. 2-gram概率计算

**答案：**
语料库：[我/爱/自然/语言/处理]，[自然/语言/处理/很/有趣]，[深度/学习/是/自然/语言/的/重要/方法]
词汇表：{我,爱,自然,语言,处理,很,有趣,深度,学习,是,的,重要,方法}，共13个词

句子：自然/语言/处理/很/有趣

2-gram概率计算：
- P(自然|<s>)：句子以"自然"开始的概率
- P(语言|自然) = C(自然,语言) / C(自然) = 1/1 = 1  
- P(处理|语言) = C(语言,处理) / C(语言) = 1/1 = 1
- P(很|处理) = C(处理,很) / C(处理) = 1/2 = 0.5
- P(有趣|很) = C(很,有趣) / C(很) = 1/1 = 1
- P(</s>|有趣)：句子以"有趣"结束的概率

假设使用加一平滑（Laplace平滑）：
P(语言|自然) = (1+1)/(1+13) = 2/14
P(处理|语言) = (1+1)/(1+13) = 2/14  
P(很|处理) = (1+1)/(2+13) = 2/15
P(有趣|很) = (1+1)/(1+13) = 2/14

P(句子) = 1/3 × 2/14 × 2/14 × 2/15 × 2/14 × 1/3 ≈ 0.00015

**详解：** n-gram基于马尔可夫假设，当前词只依赖于前n-1个词。

## 四、简答题答案

### 1. 词向量表示方法及优势

**词向量表示方法：**

**传统方法：**
- **One-Hot编码：** 每个词用一个独热向量表示，向量维度等于词典大小
- **词袋模型（Bag of Words）：** 忽略词序，统计词频

**现代词向量方法：**
- **Word2Vec：** 包括CBOW和Skip-gram两种模型
  - CBOW：根据上下文预测中心词
  - Skip-gram：根据中心词预测上下文
- **GloVe：** 全局词向量，结合全局统计信息
- **FastText：** 考虑子词信息

**数学模型（以Word2Vec为例）：**
Skip-gram: P(w_t+j | w_t) = exp(v'_{w_t+j} · v_{w_t}) / Σ_w exp(v'_w · v_{w_t})

**优势：**
1. **语义相似性：** 相似词在向量空间中距离相近
2. **向量运算：** 支持king - man + woman ≈ queen等运算
3. **维度降低：** 相比One-Hot，维度大大降低
4. **特征学习：** 自动学习词语的分布式表示
5. **泛化能力：** 有助于模型理解词的语义关系

**详解：** 词向量表示克服了传统方法无法表达语义相似性的问题。

### 2. 规则方法与统计方法比较

**规则方法：**

*优点：*
- **可解释性强：** 规则清晰明确
- **精确性高：** 在特定领域表现好
- **无数据依赖：** 不需要大量训练数据
- **错误模式可预测：** 规则违反情况明确
- **控制性强：** 可以精确控制输出

*缺点：*
- **覆盖范围有限：** 难以覆盖语言的所有情况
- **维护困难：** 规则数量多，相互冲突，维护成本高
- **扩展性差：** 增加新规则可能影响整体系统
- **语言依赖：** 不同语言需要重新制定规则
- **缺乏统计规律：** 无法学习语言的统计特性

**统计方法：**

*优点：*
- **适应性强：** 可从数据中学习语言模式
- **覆盖范围广：** 能处理训练数据中的各种情况
- **自动学习：** 无需人工制定规则
- **鲁棒性好：** 对噪声和变化有一定容忍度
- **可扩展：** 数据越多，性能越好

*缺点：*
- **可解释性差：** 模型决策过程不透明
- **需要大量数据：** 需要充足的训练数据
- **计算复杂：** 训练和推理计算量大
- **稀有情况处理差：** 罕见模式学习不足
- **过拟合风险：** 模型可能过度适应训练数据

**应用场景：**
- **规则方法：** 专业领域、语法分析、特定任务
- **统计方法：** 通用任务、大数据场景、复杂模式识别

**详解：** 现代NLP常将两种方法结合使用，发挥各自优势。

## 五、应用题答案

### 1. 中文文本分类系统设计

**1. 预处理流程：**

*文本清洗：*
- 去除HTML标签、特殊字符
- 统一编码格式
- 去除空白行和无效字符

*中文分词：*
- 使用jieba等分词工具
- 处理新词和专业术语
- 词性标注（可选）

*特征预处理：*
- 去除停用词（使用中文停用词表）
- 词形归一化（如数字统一处理）
- 构建词汇表

**2. 特征提取方法：**

*传统方法：*
- **TF-IDF：** 衡量词在文档中的重要性
- **词袋模型：** 统计词频信息
- **N-gram：** 考虑词序信息
- **特征选择：** 卡方检验、信息增益等

*深度学习方法：*
- **词向量：** 使用预训练的词向量（如Word2Vec、BERT）
- **字符级特征：** 考虑字符序列
- **句向量：** 使用预训练模型获取句子表示

**3. 模型选择：**

*传统模型：*
- **朴素贝叶斯：** 基于概率的简单模型
- **支持向量机：** 适合高维稀疏特征
- **逻辑回归：** 线性分类器，解释性强
- **随机森林：** 集成学习，鲁棒性好

*深度学习模型：*
- **CNN：** 提取局部特征，适合短文本
- **RNN/LSTM/GRU：** 处理序列信息
- **Transformer/BERT：** 最新预训练模型
- **混合模型：** 结合多种网络结构

**4. 评估指标：**

*分类评估指标：*
- **准确率（Accuracy）：** 整体分类正确的比例
- **精确率（Precision）：** 预测为正类中实际为正类的比例
- **召回率（Recall）：** 实际正类中被正确识别的比例
- **F1分数：** 精确率和召回率的调和平均
- **混淆矩阵：** 展示详细分类结果
- **ROC曲线和AUC：** 评估分类器性能

*业务指标：*
- **响应时间：** 模型推理速度
- **资源消耗：** 内存和计算资源使用
- **可扩展性：** 支持数据量增长的能力

**5. 优化策略：**

*模型优化：*
- **超参数调优：** 网格搜索、随机搜索、贝叶斯优化
- **交叉验证：** k折交叉验证评估模型稳定性
- **集成学习：** 结合多个模型的预测结果
- **模型压缩：** 减少模型大小，提高推理速度

*特征优化：*
- **特征选择：** 选择最重要的特征，减少维度
- **特征组合：** 构建更有意义的特征组合
- **词向量选择：** 使用领域相关的词向量

*数据优化：*
- **数据增强：** 通过同义词替换、回译等方法增加训练数据
- **类平衡：** 处理类别不平衡问题，使用SMOTE等方法
- **主动学习：** 选择最有价值的样本进行标注

*部署优化：*
- **模型量化：** 降低模型精度以提高推理速度
- **模型蒸馏：** 用大模型蒸馏小模型
- **缓存机制：** 缓存常见查询结果

**系统架构示例：**
```
输入 → 预处理 → 特征提取 → 分类模型 → 结果输出
                ↓
            模型管理(训练、更新、监控)
```

**详解：** 中文文本分类需要综合考虑中文语言特点、模型选择、评估指标和实际部署需求，构建完整的系统解决方案。