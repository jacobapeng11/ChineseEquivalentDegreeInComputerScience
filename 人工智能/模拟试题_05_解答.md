# 模拟试题 05 - 详细解答

## 一、选择题解答

1. **答案：C**
   - 解析：交叉操作的主要目的是组合优秀个体的特征，创造新的潜在优秀个体。

2. **答案：B**
   - 解析：一致代价搜索在有限状态空间中是完备的，只要边权为正，它总能找到解。

3. **答案：D**
   - 解析：损失函数用于衡量预测误差，不是激活函数。激活函数用于引入非线性。

4. **答案：C**
   - 解析：集中式协调需要全局信息，由中央控制器协调所有智能体。

5. **答案：B**
   - 解析：Word2Vec包含CBOW（连续词袋模型）和Skip-gram两种结构。

## 二、填空题解答

1. **答案：∀x(Bird(x) → Fly(x))**
   - 解析：使用全称量词表示对于所有x，如果是鸟，则会飞。

2. **答案：正向链式推理（Forward Chaining）；反向链式推理（Backward Chaining）**
   - 解析：专家系统的两种主要推理方式。

3. **答案：很小（接近0）；很大（明显大于训练误差）**
   - 解析：过拟合时模型在训练集上表现极好，但在测试集上表现较差。

4. **答案：信息论；信息增益 = 熵(父节点) - Σ(子节点权重 × 熵(子节点))**
   - 解析：决策树使用信息论中的熵概念计算信息增益。

5. **答案：智能行为；真正理解（意识）**
   - 解析：图灵测试验证行为智能，中文房间论证质疑理解能力。

## 三、计算题解答

### 1. A*算法求解

**题目：** 求S到G的最短路径，已知边的代价和启发函数值

**解答：**
A*算法使用f(n) = g(n) + h(n)

初始化：OPEN=[S(0+7=7)], CLOSED=[]
- 从S开始，f(S)=0+7=7
- 扩展S：添加A(1+4=5), B(4+5=9)到OPEN，CLOSED=[S]
- 选择A：f(A)=5, 扩展A：添加C(3+2=5)到OPEN，CLOSED=[S, A]
- 选择C：f(C)=5, 扩展C：添加G(8+0=8)到OPEN，CLOSED=[S, A, C]
- 选择C：f(C)=5, 检查是否需要更新，不更新
- 选择G：f(G)=8，目标到达

**答案：** 路径 S→A→C→G，总代价8

### 2. 神经网络计算

**题目：** 输入x=[2, 3]，按给定参数，使用Sigmoid激活函数

**解答：**
- 隐藏层输入：
  - h1_in = 2×0.1 + 3×0.2 + 0.1 = 0.2 + 0.6 + 0.1 = 0.9
  - h2_in = 2×0.3 + 3×0.4 + 0.1 = 0.6 + 1.2 + 0.1 = 1.9
- 隐藏层输出（Sigmoid）：
  - h1_out = 1/(1+e^(-0.9)) ≈ 0.711
  - h2_out = 1/(1+e^(-1.9)) ≈ 0.870
- 输出层计算：
  - out_in = 0.711×0.5 + 0.870×0.6 + 0.2 = 0.356 + 0.522 + 0.2 = 1.078
  - final_out = 1/(1+e^(-1.078)) ≈ 0.746

**答案：** 最终输出 ≈ 0.746

## 四、证明题解答

**题目：** 证明从"所有人类都会死"和"苏格拉底是人类"可推出"苏格拉底会死"

**证明：**
使用归结原理：
1. 知识库转换为子句：
   - 子句1：¬Human(x) ∨ Mortal(x) （所有人类都会死）
   - 子句2：Human("Socrates") （苏格拉底是人类）
   - 子句3：¬Mortal("Socrates") （待证明的否定）

2. 归结过程：
   - 步骤4：将子句1（¬Human(x) ∨ Mortal(x)）和子句2（Human("Socrates")）归结
     - 令x="Socrates"，消解Human("Socrates")
     - 得到子句：Mortal("Socrates")
   - 步骤5：将子句4（Mortal("Socrates")）和子句3（¬Mortal("Socrates")）归结
     - 消解Mortal("Socrates")
     - 得到空子句（矛盾）

3. **结论：** 从假设¬Mortal("Socrates")推出矛盾，因此Mortal("Socrates")为真

## 五、综合应用题解答

### 基于深度学习的文本情感分析系统

**1. 系统整体架构：**
- **数据输入层：** 接收原始文本数据
- **预处理层：** 分词、去停用词、文本清洗
- **特征表示层：** 词嵌入（Word Embedding）
- **模型处理层：** 深度学习模型（如LSTM/GRU/CNN）
- **分类输出层：** 情感分类（积极/消极/中性）
- **后处理层：** 结果格式化输出

**2. 文本预处理流程：**
- **文本清洗：** 去除HTML标签、特殊字符、URL等
- **分词处理：** 中英文分词，处理未登录词
- **去停用词：** 移除无意义词汇
- **词形还原：** 将词汇转换为原型
- **序列化：** 将文本转换为数值序列
- **填充截断：** 统一序列长度

**3. 模型设计：**
- **嵌入层：** 将词索引转换为词向量
- **LSTM层：** 捕捉文本序列的长期依赖关系
- **注意力机制：** 关注情感关键词
- **全连接层：** 特征组合
- **Softmax层：** 输出情感类别概率

**4. 评估指标和优化策略：**
- **评估指标：** 准确率、精确率、召回率、F1分数
- **优化策略：** 
  - 使用预训练词向量（Word2Vec/GloVe）
  - 正则化防止过拟合
  - 学习率调度
  - 数据增强

## 六、简答题解答

**SVM与神经网络比较分析：**

### 支持向量机（SVM）：
**优点：**
- 在高维空间中有效
- 内存效率高（仅支持向量重要）
- 通过核函数处理非线性问题
- 泛化能力强，不易过拟合
- 提供最佳分割超平面

**缺点：**
- 大数据集训练时间长
- 对噪声和异常值敏感
- 不直接提供概率估计
- 核函数和参数选择需要经验

### 神经网络：
**优点：**
- 强大的非线性建模能力
- 自动特征学习
- 适用于复杂模式识别
- 可处理大规模数据
- 架构灵活，易于调整

**缺点：**
- 需要大量训练数据
- 容易过拟合
- 训练时间长，计算复杂
- 参数调整困难
- 可解释性差

### 选择策略：
**选择SVM：**
- 小到中等规模数据集（<10000样本）
- 高维稀疏数据（如文本分类）
- 需要清晰决策边界
- 对模型解释性要求较高

**选择神经网络：**
- 大规模数据集
- 复杂非线性关系
- 图像识别、语音识别等任务
- 需要特征自动学习
- 有充足计算资源

**应用场景：**
- SVM：邮件分类、文本分类、生物信息学
- 神经网络：图像识别、自然语言处理、语音识别